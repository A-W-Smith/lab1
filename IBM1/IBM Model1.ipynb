{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".output_wrapper, .output {\n",
    "    height:auto !important;\n",
    "    max-height:10000px;\n",
    "}\n",
    ".output_scroll {\n",
    "    box-shadow:none !important;\n",
    "    webkit-box-shadow:none !important;\n",
    "    \n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This cell needs to be executed first for the initial\n",
    "# visualization to work. All important functions used here\n",
    "# are explained later.\n",
    "\n",
    "from IPython.display import HTML\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.collections import LineCollection\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from copy import deepcopy, copy\n",
    "%matplotlib inline\n",
    "\n",
    "data_file = \"data/dev-test-train.pl-en\"\n",
    "num_sent = sys.maxint\n",
    "num_iter = 10\n",
    "\n",
    "def get_bitext(file_name, num_sent):\n",
    "    return [[sentence.strip().split() for sentence in pair.split(' ||| ')] for pair in open(file_name)][:num_sent]\n",
    "\n",
    "def train(bitext, num_iter):\n",
    "    k = 0\n",
    "    theta = initialize_theta(bitext)\n",
    "    print(\"\\nInitialization:\\n\")\n",
    "    while k < num_iter:\n",
    "        k += 1\n",
    "        theta = iteration(theta, bitext)\n",
    "        print(\"\\nIteration {}:\\n\".format(str(k)))\n",
    "    return theta\n",
    "\n",
    "def initialize_theta(bitext):\n",
    "    f_vocab = set()\n",
    "    e_vocab = set()\n",
    "    for (f, e) in bitext:\n",
    "        f_vocab.update(f)\n",
    "        e_vocab.update(e)\n",
    "    theta = {}\n",
    "    default_p = 1.0/len(f_vocab)\n",
    "    for e in e_vocab:\n",
    "        theta[e] = defaultdict(float)\n",
    "        for f in f_vocab:\n",
    "            theta[e][f] = default_p\n",
    "    return theta\n",
    "\n",
    "def expectation(theta, bitext):\n",
    "    e_count = defaultdict(float)\n",
    "    fe_count = defaultdict(float)\n",
    "    for (n, (f, e)) in enumerate(bitext):\n",
    "        for f_i in f:\n",
    "            z = 0\n",
    "            for e_j in e:\n",
    "                z += theta[e_j][f_i]\n",
    "            for e_j in e:\n",
    "                c = theta[e_j][f_i] / z\n",
    "                fe_count[(f_i, e_j)] += c\n",
    "                e_count[e_j] += c\n",
    "    return e_count, fe_count\n",
    "\n",
    "def maximization(theta, e_count, fe_count):\n",
    "    new_theta = deepcopy(theta)\n",
    "    for e_i in new_theta:\n",
    "        for f_j in new_theta[e_i]:\n",
    "            if (f_j, e_i) in fe_count:\n",
    "                new_theta[e_i][f_j] = fe_count[(f_j, e_i)]/e_count[e_i]\n",
    "            else:\n",
    "                new_theta[e_i][f_j] = 0\n",
    "    return new_theta\n",
    "\n",
    "def iteration(theta, bitext):\n",
    "    e_count, fe_count = expectation(theta, bitext)\n",
    "    new_theta = maximization(theta, e_count, fe_count)\n",
    "    return new_theta\n",
    "\n",
    "def align(f_sent, e_sent, theta):\n",
    "    a = []\n",
    "    for i in range(0, len(f_sent)):\n",
    "        best_prob = 0\n",
    "        best_j = 0\n",
    "        for j in range(0, len(e_sent)):\n",
    "            if theta[e_sent[j]][f_sent[i]] > best_prob:\n",
    "                best_prob = theta[e_sent[j]][f_sent[i]]\n",
    "                best_j = j\n",
    "        a.append((i, best_j))\n",
    "    return a\n",
    "\n",
    "#######################################\n",
    "# STATIC ILUSTRATIONS\n",
    "#######################################\n",
    "def get_thetas(bitext, num_iter):\n",
    "    k = 0\n",
    "    thetas=[]\n",
    "    theta = initialize_theta(bitext)\n",
    "    thetas.append(theta)\n",
    "    while k < num_iter:\n",
    "        k += 1\n",
    "        new_theta = iteration(theta, bitext)\n",
    "        theta = new_theta\n",
    "        thetas.append(new_theta)\n",
    "    return thetas\n",
    "\n",
    "def show_alignments(initialize=initialize_theta):\n",
    "    train_and_draw(bitext, num_iter, draw_a=True, initialize=initialize)\n",
    "\n",
    "def show_translation_tables(initialize=initialize_theta):\n",
    "    train_and_draw(bitext, num_iter, draw_p=True, initialize=initialize)\n",
    "    \n",
    "def show_sent_data(at_iter, sent_index=0, word_index=0, show_translation_p=True):\n",
    "    if sent_index > len(bitext)-1:\n",
    "        sent_index = len(bitext)-1\n",
    "    if word_index > len(bitext[sent_index][0])-1:\n",
    "        word_index = len(bitext[sent_index][0])-1\n",
    "#     theta = train(bitext, at_iter)\n",
    "    theta = thetas20[at_iter]\n",
    "    f, e = bitext[sent_index]\n",
    "    coords = get_coordinates(bitext, one_sent=True, sent_index=sent_index, word_index=word_index)\n",
    "    annot1_y = [y+0.5 for y in coords['y_e']]\n",
    "    annot1_y.append(annot1_y[0])\n",
    "    if show_translation_p:\n",
    "        annot2_y = [y+1 for y in coords['y_e']]\n",
    "        annot2_y.append(annot2_y[0])\n",
    "    annot_x = [0]\n",
    "    annot_x.extend(coords['x_e'])\n",
    "    annot_color = ['#1a75ff']\n",
    "    annot_color.extend(['#000000']*len(coords['x_e']))\n",
    "    alignment_p = ['{:.2f}'.format(p) for p in \n",
    "                   get_alignment_probabilities(f, e, theta, one_word=True, word_index=word_index)[0]]\n",
    "    annot1_words = ['alignment\\nprobability']\n",
    "    annot1_words.extend(alignment_p)\n",
    "    if show_translation_p:\n",
    "        translation_p = ['{:.2f}'.format(p) for p in \n",
    "                         get_translation_probabilities(f, e, theta, one_word=True, word_index=word_index)[word_index]]\n",
    "        annot2_words = ['translation\\nprobability']\n",
    "        annot2_words.extend(translation_p)\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = plt.axes()\n",
    "    plt.axis('off')\n",
    "    if show_translation_p:\n",
    "        ax.scatter(coords['x_f']+coords['x_e']+annot_x*2,\n",
    "                   coords['y_f']+coords['y_e']+annot1_y+annot2_y,\n",
    "                   s=30, c='white', marker='o', lw=0, alpha=1)\n",
    "        plot_words(ax, annot_x, annot2_y, annot2_words, 'bottom', weight='normal', color=annot_color)\n",
    "    else:\n",
    "        ax.scatter(coords['x_f']+coords['x_e']+annot_x,\n",
    "                   coords['y_f']+coords['y_e']+annot1_y,\n",
    "                   s=30, c='white', marker='o', lw=0, alpha=1)\n",
    "    plot_words(ax, coords['x_f'], coords['y_f'], coords['w_f'], 'top')\n",
    "    plot_words(ax, coords['x_e'], coords['y_e'], coords['w_e'], 'bottom')\n",
    "    plot_words(ax, annot_x, annot1_y, annot1_words, 'bottom', weight='normal', color=annot_color)\n",
    "#     plot_words(ax, annot_x, annot2_y, annot2_words, 'bottom', weight='normal', color=annot_color)\n",
    "    \n",
    "    w_start = len(f)*word_index\n",
    "    raw_line_weights = get_line_weights([bitext[sent_index]], at_iter, thetas=[theta])[0]\n",
    "    line_weights = [w*10 for w in raw_line_weights][w_start:(w_start+len(f))]\n",
    "    edge_coords = coords['edges']\n",
    "    lines = [ax.plot(xy[0], xy[1],alpha=0.9,lw=w,linestyle='-',color='#1a75ff')[0] for xy,w\n",
    "             in zip(coords['edges'], line_weights)]\n",
    "    plt.show()\n",
    "\n",
    "def draw_iteration(at_iter, bitext):\n",
    "    theta_before = thetas20[at_iter-1]\n",
    "    theta_after = thetas20[at_iter]\n",
    "    f_order = theta_before.values()[0].keys()\n",
    "    fig = plt.figure(1, figsize=(14, 10))\n",
    "    \n",
    "    # first translation table\n",
    "    ax1 = plt.subplot(221)\n",
    "    draw_translation_table(bitext, theta_before, f_order, fig=ax1)\n",
    "    plt.title('Parameters  from last iteration', color='#1a75ff', y=1.08)\n",
    "    \n",
    "    # alignment graph\n",
    "    ax2 = plt.subplot(222)\n",
    "    line_weights = [[w*(10.0/2) for w in sublist] for sublist in get_line_weights(bitext, at_iter-1)]\n",
    "    coordinates = get_coordinates(bitext)\n",
    "    ax2.axis('off')\n",
    "    ax2.scatter(coordinates['x_f']+coordinates['x_e'], coordinates['y_f']+coordinates['y_e'],\n",
    "           s=30, c='white', marker='o', lw=0,alpha=1)\n",
    "    plot_words(ax2, coordinates['x_f'], coordinates['y_f'], coordinates['w_f'], 'top')\n",
    "    plot_words(ax2, coordinates['x_e'], coordinates['y_e'], coordinates['w_e'], 'bottom')\n",
    "    lines = [ax2.plot(xy[0], xy[1],alpha=0.9,linestyle='-',lw=w,color='#b20000')[0]\n",
    "             for xy,w in zip(coordinates['edges'],line_weights[at_iter-1])]\n",
    "    plt.title('Alignment probabilities and expected counts', color='#b20000')\n",
    "    \n",
    "    # expected alignment counts\n",
    "    ax3 = plt.subplot(224)\n",
    "    draw_expected_counts(bitext, theta_before, f_order, fig=ax3)\n",
    "#     plt.title('Expected alignment counts', color='#b20000', y=1.08)\n",
    "    \n",
    "    # second translation table\n",
    "    ax4 = plt.subplot(223)\n",
    "    draw_translation_table(bitext, theta_after, f_order, fig=ax4)\n",
    "    plt.title('Updated parameters', color='#1a75ff', y=1.08)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def train_and_draw(bitext, num_iter, draw_a=False, draw_p=False, initialize=initialize_theta):\n",
    "    k = 0\n",
    "    theta = initialize(bitext)\n",
    "    f_order = theta.values()[0].keys()\n",
    "    if draw_a or draw_p:\n",
    "        print(\"\\nInitialization:\\n\")\n",
    "    draw(bitext, theta, draw_a, draw_p, f_order)\n",
    "    while k < num_iter:\n",
    "        k += 1\n",
    "        new_theta = iteration(theta, bitext)\n",
    "        theta = new_theta\n",
    "        if draw_a or draw_p:\n",
    "            print(\"\\nIteration {}:\\n\".format(str(k)))\n",
    "        draw(bitext, theta, draw_a, draw_p, f_order)\n",
    "    return theta\n",
    "\n",
    "def draw(bitext, theta, draw_a, draw_p, f_order):\n",
    "    if draw_p:\n",
    "        draw_translation_table(bitext, theta, f_order)\n",
    "    if draw_a:\n",
    "        draw_alignments(bitext, theta)\n",
    "    \n",
    "def draw_alignments(bitext, theta, fig=None, scale=None):\n",
    "    if not fig:\n",
    "        plt.figure(figsize=(10, 15))\n",
    "#     font_size = 18 if not scale else 18.0*scale\n",
    "    node_size = 3000 if not scale else 3000.0*scale\n",
    "    for (n, (f, e)) in enumerate(bitext):\n",
    "        a_graph = nx.Graph()\n",
    "        a_graph.add_nodes_from(e, bipartite=0)\n",
    "        a_graph.add_nodes_from(f, bipartite=1)\n",
    "        a_probs = get_alignment_probabilities(f, e, theta)\n",
    "        pos = {}\n",
    "        for j in range(0, len(f)):\n",
    "            for i in range(0, len(e)):\n",
    "                a_graph.add_edge(f[j], e[i], weight=a_probs[j][i])\n",
    "            pos[f[j]] = (j+1, (2*n) - 1)\n",
    "        for i in range(0, len(e)):\n",
    "            pos[e[i]] = (i+1, 2*n)\n",
    "        nodes = nx.draw_networkx_nodes(a_graph, pos, node_size=node_size, alpha=1, node_color='white')\n",
    "        nodes.set_edgecolor('white')\n",
    "        nx.draw_networkx_labels(a_graph, pos, font_size=18, font_family='sans-serif',\n",
    "                                font_weight='bold', font_color='black')\n",
    "        edge_weights = [int(attr['weight']*100) for (_,_, attr) in a_graph.edges(data=True)]\n",
    "        nx.draw_networkx_edges(a_graph, pos, width=3, alpha=0.9, edge_color=edge_weights,\n",
    "                               edge_cmap=plt.cm.Blues)\n",
    "    plt.axis('off')\n",
    "    if not fig:\n",
    "        plt.show()\n",
    "\n",
    "def draw_translation_table(bitext, theta, f_order=None, fig=None):\n",
    "    if not fig:\n",
    "        plt.figure(figsize=(7, 5))\n",
    "    if not f_order:\n",
    "        f_order = theta.values()[0].keys()\n",
    "    theta_arrays = {e: [theta[e][f] for f in f_order] for e in theta.keys()}\n",
    "    theta2frame = pd.DataFrame.from_dict(theta_arrays, orient=\"index\")\n",
    "    theta2frame.columns = f_order\n",
    "    data = theta2frame.round(2)\n",
    "    sns.set(font_scale=1.2)\n",
    "    sns.set_style({\"savefig.dpi\": 100})\n",
    "    ax = sns.heatmap(data, cmap=plt.cm.Blues, linewidths=.1, annot=True, fmt=\"2g\")\n",
    "    ax.xaxis.tick_top()\n",
    "    plt.yticks(rotation=0)\n",
    "    if not fig:\n",
    "        plt.show()\n",
    "        \n",
    "def draw_expected_counts(bitext, theta, f_order=None, fig=None):\n",
    "    if not fig:\n",
    "        plt.figure(figsize=(7, 5))\n",
    "    if not f_order:\n",
    "        f_order = theta.values()[0].keys()\n",
    "    _, fe_count = expectation(theta, bitext)\n",
    "    fe_dict = {}\n",
    "    for f in theta.keys():\n",
    "        fe_dict[f] = {}\n",
    "        for e in theta.values()[0].keys():\n",
    "            fe_dict[f][e] = fe_count[(e,f)] if (e,f) in fe_count else 0        \n",
    "    counts_arrays = {e: [fe_dict[e][f] for f in f_order] for e in fe_dict.keys()}\n",
    "    counts2frame = pd.DataFrame.from_dict(counts_arrays, orient=\"index\")\n",
    "    counts2frame.columns = f_order\n",
    "    data = counts2frame.round(2)\n",
    "    sns.set(font_scale=1.2)\n",
    "    sns.set_style({\"savefig.dpi\": 100})\n",
    "    ax = sns.heatmap(data, cmap=plt.cm.Reds, linewidths=.1, annot=True, fmt=\"2g\")\n",
    "    ax.xaxis.tick_top()\n",
    "    plt.yticks(rotation=0)\n",
    "    if not fig:\n",
    "        plt.show()\n",
    "    \n",
    "def get_translation_probabilities(f, e, theta, one_word=False, word_index=0):\n",
    "    t_probs = []\n",
    "    for j in range(0, len(f)):\n",
    "        t_probs.append([])\n",
    "        if (not one_word) or (one_word and word_index==j):\n",
    "#             t_probs.append([])\n",
    "            for i in range(0, len(e)):\n",
    "                t_probs[j].append(theta[e[i]][f[j]])\n",
    "    return t_probs\n",
    "\n",
    "def get_alignment_probabilities(f, e, theta, one_word=False, word_index=0):\n",
    "    t_probs = get_translation_probabilities(f, e, theta, one_word, word_index)\n",
    "    A = []\n",
    "    for i in range(0, len(t_probs)):\n",
    "        if t_probs[i]:\n",
    "            total = sum(t_probs[i])\n",
    "            alignment_probs = [x/total for x in t_probs[i]]\n",
    "            A.append(alignment_probs)\n",
    "    return A\n",
    "\n",
    "def plot_likelihoods(initialize=initialize_theta, threshold=0.01):\n",
    "#     bitext = get_bitext(data_file, num_sent)\n",
    "    y = get_likelihoods(bitext, initialize(bitext), threshold)\n",
    "    x = range(0, len(y))\n",
    "    plt.figure(figsize=(6, 4.5))\n",
    "    plt.plot(x, y, marker='o', markersize=5, color='cornflowerblue', linestyle='-', linewidth=2)\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('log likelihood')\n",
    "    plt.show()\n",
    "\n",
    "def get_likelihoods(bitext, theta, threshold, previous_likelihood=-sys.maxint, l=[]):\n",
    "#     threshold = 0.01\n",
    "    likelihood = get_data_log_likelihood(bitext, theta)\n",
    "    l.append(likelihood)\n",
    "    if (likelihood - previous_likelihood) > threshold:\n",
    "        new_theta = iteration(theta, bitext)\n",
    "        return get_likelihoods(bitext, new_theta, threshold, likelihood, l)\n",
    "    else:\n",
    "        return l\n",
    "\n",
    "#######################################\n",
    "# PRINTING\n",
    "#######################################\n",
    "\n",
    "def print_iteration(k, bitext, theta):\n",
    "#     print(\"\\nIteration {}:\\n\".format(str(k)))\n",
    "    likelihood = get_data_log_likelihood(bitext, theta)\n",
    "    print(\"Data log likelihood is {}\\n\".format(str(likelihood)))\n",
    "    print(\"Translation probabilities:\")\n",
    "    for (f, e) in bitext:\n",
    "        t_probs = get_translation_probabilities(f, e, theta)\n",
    "        headings = [\"\"]\n",
    "        headings.extend([\"e\"+str(n)+\": \"+word for (n, word) in enumerate(e)])\n",
    "        out_t = PrettyTable(headings, hrules=True)\n",
    "        for j in range(0, len(f)):\n",
    "            row = [\"f\"+str(j)+\": \"+f[j]]\n",
    "            row.extend([\"%.2f\" %p for p in t_probs[j]])\n",
    "            out_t.add_row(row)\n",
    "        out_t.align[\"\"] = \"l\"\n",
    "        print(out_t)\n",
    "\n",
    "def print_translation_tables(k, theta):\n",
    "    e_vocab = theta.keys()\n",
    "    f_vocab = set([])\n",
    "    [f_vocab.update(set(d.keys())) for d in theta.values()]\n",
    "    f_vocab = list(f_vocab)\n",
    "    headings = [\"\"]\n",
    "    headings.extend(f_vocab)\n",
    "    out_t = PrettyTable(headings, hrules=True)\n",
    "    for e in e_vocab:\n",
    "        row = [e]\n",
    "        e_trans = theta[e]\n",
    "        for f in f_vocab:\n",
    "            if f in e_trans:\n",
    "                row.append(\"%.2f\" %e_trans[f])\n",
    "            elif k != 0:\n",
    "                row.append(\"0.00\")\n",
    "            else:\n",
    "                row.append(\"%.2f\" %(1.0/len(f_vocab)))\n",
    "        out_t.add_row(row)\n",
    "    out_t.align[\"\"] = \"l\"\n",
    "    print(out_t)\n",
    "\n",
    "#######################################\n",
    "# ANIMATIONS\n",
    "#######################################\n",
    "def init_alignments():\n",
    "    ax.scatter(coordinates['x_f']+coordinates['x_e'], coordinates['y_f']+coordinates['y_e'],\n",
    "           s=30, c='white', marker='o', lw=0,alpha=1)\n",
    "    plot_words(ax, coordinates['x_f'], coordinates['y_f'], coordinates['w_f'], 'top')\n",
    "    plot_words(ax, coordinates['x_e'], coordinates['y_e'], coordinates['w_e'], 'bottom')\n",
    "    for (n, line) in enumerate(lines):\n",
    "        line.set_linewidth(line_weights[0][n])\n",
    "    return lines\n",
    "\n",
    "def animate_alignments(i):\n",
    "    for (n, line) in enumerate(lines):\n",
    "        line.set_linewidth(line_weights[i][n])\n",
    "    return lines\n",
    "\n",
    "def get_line_weights(bitext, num_iter, thetas=None):\n",
    "    if not thetas:\n",
    "        thetas = get_thetas(bitext, num_iter)\n",
    "    weights = []\n",
    "    for theta in thetas:\n",
    "        iteration_weights=[]\n",
    "        for f, e in bitext:\n",
    "            a_probs = get_alignment_probabilities(f, e, theta)\n",
    "            for j in range(0, len(f)):\n",
    "                for i in range(0, len(e)):\n",
    "                    iteration_weights.append(a_probs[j][i])\n",
    "        weights.append(iteration_weights)\n",
    "    return weights\n",
    "\n",
    "def get_coordinates(bitext, one_sent=False, sent_index=0, word_index=0):\n",
    "    x_positions_f = []\n",
    "    y_positions_f = []\n",
    "    x_positions_e = []\n",
    "    y_positions_e = []\n",
    "    edge_pos = []\n",
    "    words_f = []\n",
    "    words_e = []\n",
    "    sents = bitext if not one_sent else [bitext[sent_index]]\n",
    "    for (n, (f, e)) in enumerate(sents):\n",
    "        for j in range(0, len(f)):\n",
    "            x_positions_f.append(j+1)\n",
    "            y_positions_f.append((3*n)-2)\n",
    "            words_f.append(f[j])\n",
    "            if (not one_sent) or (one_sent and word_index==j):\n",
    "                for i in range(0, len(e)):\n",
    "                    edge_pos.append([[j+1, i+1], [(3*n)-1.9, (3*n)-1.1]])\n",
    "        for i in range(0, len(e)):\n",
    "            x_positions_e.append(i+1)\n",
    "            y_positions_e.append((3*n)-1)\n",
    "            words_e.append(e[i])\n",
    "    coord_dict = {'x_f': x_positions_f, 'x_e': x_positions_e,\n",
    "            'y_f': y_positions_f, 'y_e': y_positions_e,\n",
    "            'edges': edge_pos, 'w_f': words_f, 'w_e': words_e}\n",
    "    return coord_dict\n",
    "\n",
    "def plot_words(axes, xs, ys, words, vertical_position, weight='bold', color='black'):\n",
    "    for n in range(0, len(words)):\n",
    "        word = words[n]\n",
    "        x = xs[n]\n",
    "        y = ys[n]\n",
    "        if isinstance(color, list):\n",
    "            current_color=color[n]\n",
    "        else:\n",
    "            current_color=color\n",
    "        axes.text(x, y, word, size=15, family='sans-serif', weight=weight, color=current_color,\n",
    "                  horizontalalignment='center',\n",
    "                  verticalalignment=vertical_position)\n",
    "\n",
    "\n",
    "# hide this cell\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    "    if (code_show){\n",
    "        $('div.cell.code_cell.rendered.selected div.input').hide();\n",
    "    } else {\n",
    "        $('div.cell.code_cell.rendered.selected div.input').show();\n",
    "    }\n",
    "    code_show = !code_show\n",
    "} \n",
    "\n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "\n",
    "To show/hide code in this cell, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bitext = get_bitext(data_file, num_sent)\n",
    "thetas20 = get_thetas(bitext, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Word Alignment and the Expectation-Maximization Algorithm\n",
    "======\n",
    "\n",
    "We will implement IBM Model 1 and, working on a toy dataset, step through it to get a feeling for how Expectation-Maximization works.\n",
    "\n",
    "Our aim is to model the conditional probability of a Foreign sentence F given an English sentence E - $p\\theta(F|E)$, where $\\theta$ denotes model parameters. We're going to make the assumption that each word $f \\in F$ is a translation of one word $e \\in E$. Those links between source and target words are alignments - a latent variable of IBM Model 1. They are stipulated, not given in the data. The only data we have available are paired sentences, one being the translation of the other.\n",
    "\n",
    "If we knew the correct alignment, $p\\theta(F|E)$ would be the product over $f \\in F$ of the probability of $f$ being the translation of the aligned word $e$. Translation probabilities are parameters of our model which we want to discover.\n",
    "\n",
    "Were the alignments observed rather than hidden, parameter setting could proceed through simple Maximum Likelihood Estimation. We would collect co-occurence counts between aligned words and calculate translation probabilities. With latent alignments, however, we're going to make use of the Expectation Maximization approach. We'll make a guess about translation probabilities, and use that guess to establish expected alignments. Using those alignments we'll improve our parameters. So, in each iteration we take our current guess about translation probabilities, and update it by making use of the model's latent variable.\n",
    "\n",
    "Let's visualize how alignment probabilities change as we train the model on a toy dataset (the heavier the line, the higher the probability):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "line_weights = [[w*10 for w in sublist] for sublist in get_line_weights(bitext, num_iter, thetas20)]\n",
    "coordinates = get_coordinates(bitext)\n",
    "fig = plt.figure(figsize=(8, 12))\n",
    "ax = plt.axes()\n",
    "plt.axis('off')\n",
    "lines = [ax.plot(xy[0], xy[1],alpha=0.9,linestyle='-',color='#1a75ff')[0] for xy in coordinates['edges']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "anim = FuncAnimation(fig, animate_alignments, init_func=init_alignments, frames=num_iter, interval=1000,\n",
    "                     blit=True, repeat=False)\n",
    "HTML(anim.to_html5_video())\n",
    "\n",
    "# anim.save('alignment.gif', writer='imagemagick', fps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also observe how translation probabilities change over iterations. Each row represents translation probability distribution over vocabulary of F for one word from vocabulary of E.\n",
    "\n",
    "TODO: should be an animation, not a list of pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# thetas = get_thetas(bitext, num_iter)\n",
    "# fig = plt.figure(figsize=(7, 5))\n",
    "# sns.set(font_scale=1.2)\n",
    "# sns.set_style({\"savefig.dpi\": 100})\n",
    "# theta2frame = pd.DataFrame.from_dict(thetas[0], orient=\"index\")\n",
    "# data = theta2frame.round(2)\n",
    "# # plot = sns.heatmap(data, cmap=plt.cm.Blues, linewidths=.1, annot=True, fmt=\"2g\")\n",
    "# ax = sns.heatmap(data, cmap=plt.cm.Blues, linewidths=.1)\n",
    "# ax.xaxis.tick_top()\n",
    "# plt.yticks(rotation=0)\n",
    "    \n",
    "# def init():\n",
    "#     ax.data = data\n",
    "#     return ax,\n",
    "\n",
    "# def animate(i):\n",
    "#     theta2frame = pd.DataFrame.from_dict(thetas[i], orient=\"index\")\n",
    "#     data = theta2frame.round(2)\n",
    "#     ax.data = data\n",
    "#     return ax,\n",
    "\n",
    "# anim = FuncAnimation(fig, animate, init_func=init, frames=num_iter, interval=1000, repeat_delay=10000)\n",
    "# HTML(anim.to_html5_video())\n",
    "# anim.save('translation.gif', writer='imagemagick', fps=2) \n",
    "\n",
    "show_translation_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "           QUESTION\n",
    "    why doesn't the model learn to align '.' to '.' and 'sa' to 'are'? \n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, let us go back to iteration 0, when we know nothing about the actual translation probabilities and correct alignments. In absence of reasons to think otherwise, we will guess that translation probability distributions are uniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def initialize_theta(bitext):\n",
    "    f_vocab = set()\n",
    "    e_vocab = set()\n",
    "    for (f, e) in bitext:\n",
    "        f_vocab.update(f)\n",
    "        e_vocab.update(e)\n",
    "    theta = {}\n",
    "    default_p = 1.0/len(f_vocab)\n",
    "    for e in e_vocab:\n",
    "        theta[e] = defaultdict(float)\n",
    "        for f in f_vocab:\n",
    "            theta[e][f] = default_p\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having made an initial guess, we can now execute the Expectation step of our EM learning algorithm. Given the translation probability table we can say what is our expectation about the alignments. The intuition is that for words *f* and *e* co-occuring in a sentence pair, if word *f* is a highly probable translation of word *e*, it is likely that *f* is aligned to *e*. In fact, the probability of alignment is proportional to translation probability. You can inspect translation and alignment probabilities for any Foreign word token by changing the iteration, sentence, and word index argumens in the function call below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_sent_data(at_iter=0, sent_index=1, word_index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Expectation step we use alignmment probabilities as weights on word co-occurences. In the example above the expected co-occurence count between *buty* and each of the four *e* words is 0.25.\n",
    "\n",
    "We go through the whole corpus and collect all the expected co-occurence counts, as well as the occurence counts for each word in the English vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def expectation(theta, bitext):\n",
    "    e_count = defaultdict(float)\n",
    "    fe_count = defaultdict(float)\n",
    "    for (n, (f, e)) in enumerate(bitext):\n",
    "        for f_i in f:\n",
    "            z = 0\n",
    "            for e_j in e:\n",
    "                z += theta[e_j][f_i]\n",
    "            for e_j in e:\n",
    "                c = theta[e_j][f_i] / z\n",
    "                fe_count[(f_i, e_j)] += c\n",
    "                e_count[e_j] += c\n",
    "    return e_count, fe_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Maximization step we use the expected counts as we would use actual counts in MLE and generate a new translation probability table. In other words, we change our initial belief about the parameters based on newly acquired belief about alignments.\n",
    "\n",
    "For instance, *shoes* occurs once in our corpus, and co-occurs 0.25 times with *buty, nie, są*, and *smaczne*. We will re-evaluate translation probability $p_{t}(f | e)$ as 0.25 for the above four *f* words and 0 for the rest of the Foreign vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def maximization(theta, e_count, fe_count):\n",
    "    new_theta = deepcopy(theta)\n",
    "    for e_i in new_theta:\n",
    "        for f_j in new_theta[e_i]:\n",
    "            if (f_j, e_i) in fe_count:\n",
    "                new_theta[e_i][f_j] = fe_count[(f_j, e_i)]/e_count[e_i]\n",
    "            else:\n",
    "                new_theta[e_i][f_j] = 0\n",
    "    return new_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those two steps together constitute one iteration of the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def iteration(theta, bitext):\n",
    "    e_count, fe_count = expectation(theta, bitext)\n",
    "    new_theta = maximization(theta, e_count, fe_count)\n",
    "    return new_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To illustrate the parameter updating which happens during an iteration, we can look at the translation probabilities from the previous iteration, the expected alignments derived on their basis, and the translation probabilities for this iteration , derived from the expected alignments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "draw_iteration(2, bitext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to execute an iteration of EM training, it's time to ask how many iterations do we need to train our model. In the limit, EM on IBM Model 1 will converge to the true maximum likelihood estimate, a fact we know because of two important properties of EM: that the likelihood of $\\theta_{i}$ will always be higher than the likelihood of $\\theta_{i-1}$, and that the likelihood function of Model 1 is convex. (The only requirement for reaching the global optimum is that none of the initial parameters is 0.) So in theory, the answers is that you should run it for “a long time”. The empirical answer is somewhat different: most people run EM for a small fixed number of iterations, usually only three to five."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train1(bitext, num_iter):\n",
    "    k = 0\n",
    "    theta = initialize_theta(bitext)\n",
    "    print(\"\\nInitialization:\\n\")\n",
    "    draw_translation_table(k, bitext, theta)\n",
    "    while k < num_iter:\n",
    "        k += 1\n",
    "        theta = iteration(theta, bitext)\n",
    "        print(\"\\nIteration {}:\\n\".format(str(k)))\n",
    "        draw_translation_table(k, bitext, theta)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of fixing the iteration number we could check how much better does the model perform after each iteration, and decide to stop if the improvement is below a set threshold.  A reasonable measure of model performance is data likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train2(bitext, previous_likelihood=-sys.maxint):\n",
    "    threshold = 0.01\n",
    "    theta = initialize_theta(bitext)\n",
    "    likelihood = get_data_log_likelihood(bitext, theta)\n",
    "    while (likelihood - previous_likelihood) > threshold:\n",
    "        theta = iteration(theta, bitext)\n",
    "        likelihood = get_data_log_likelihood(bitext, theta)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*************************************************\n",
    "           TASK 1\n",
    "    write a function which takes bitext and model parameters as input and returns the bitext log likelihood under\n",
    "    the parameters as output\n",
    "************************************************* \n",
    "\n",
    "We provide you with a log_add function, which returns $ln(x+y)$ when given $ln(x)$ and $ln(y)$. The check if your implementation is correct, execute the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_data_log_likelihood(bitext, theta):\n",
    "    #***\n",
    "    #your code\n",
    "    #***\n",
    "    return data_log_likelihood\n",
    "\n",
    "def log_add(x,y):\n",
    "    # given x=ln(x') and y=ln(y') returns ln(x'+y')\n",
    "    return x + np.log(1+ np.exp(y-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_data_log_likelihood(bitext, thetas20[0]) == -1.6218604324326578"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO REMOVE\n",
    "\n",
    "def get_data_log_likelihood(bitext, theta):\n",
    "    data_log_likelihood = 0\n",
    "    for (f, e) in bitext:\n",
    "        data_log_likelihood += get_pair_likelihood(e,f,theta)\n",
    "        return data_log_likelihood\n",
    "    \n",
    "def get_pair_likelihood(e_sent,f_sent,theta):\n",
    "    first_column = [np.log(theta[e_j][f_sent[0]]) for e_j in e_sent]\n",
    "    current_sum = list_log_add(first_column)\n",
    "    for i in range(1, len(f_sent)):\n",
    "        next_column = [(np.log(theta[e_j][f_sent[i]]) + current_sum) for e_j in e_sent]\n",
    "        next_sum = list_log_add(next_column)\n",
    "        current_sum = next_sum\n",
    "    return current_sum\n",
    "\n",
    "def list_log_add(l):\n",
    "    if len(l) == 1:\n",
    "        return l[0]\n",
    "    else:\n",
    "        new_l = []\n",
    "        first_sum = log_add(l[0], l[1])\n",
    "        new_l.append(first_sum)\n",
    "        new_l.extend(l[2:])\n",
    "        return list_log_add(new_l)\n",
    "\n",
    "def log_add(x,y):\n",
    "    # given x=ln(x') and y=ln(y') returns ln(x'+y')\n",
    "    return x + np.log(1+ np.exp(y-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having written get_data_log_likelihood() you can inspect how log likelihood changes over iterations and visualize why a small number of iterations might be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_likelihoods(threshold=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "*************************************************\n",
    "           TASK 2\n",
    "    write a new θ initialization function\n",
    "************************************************* \n",
    "In thory, whatever the initial $theta$, EM will converge on the true MLE estimate, which makes our simple uniform initializatio adequate. In practice, [Moore(2004)] shows that using heuristic estimates to set initial parameter values can significantly improve final model accuracy.\n",
    "\n",
    "Your task is to experiment, but not necessarily improve model performance. Try to implement an initialization method that seems sensible to you, and make observations about how and translation probabilitie and data likelihood evolve over iterations, as compared to when uniform initialization is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def initialize_theta2(bitext):\n",
    "    #***\n",
    "    #your code\n",
    "    #***\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_translation_tables(initialize=initialize_theta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_likelihoods(threshold=0.01, initialize=initialize_theta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*************************************************\n",
    "           TASK 3\n",
    "    make null alignments possible\n",
    "************************************************* \n",
    "So far we've assumed that each Foreign word is aligned to exactly one English word. However, it's easy to come up with sentence pairs in which for some foreign word there is no source on the English side. For instance, the Polish question particle *czy* in *czy to jest smaczne?* has no equivalent in *is it tasty?*. If we forced *czy* to align to one of the English words we would be in error. Instead, we can append a null token onto the English sentence, to which all target words with no clear source equaivalent can align.\n",
    "\n",
    "You task is to modify the model to accomodate null alignment. You should be able to prepare an animation of the training process using the code for the animation we provided at the beginning of this lab.  \n",
    "\n",
    "To inspect the behaviour of your new model, you should change the data file to data/dev-test-train-null.pl-en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*************************************************\n",
    "           EXTRA ACTIVITIES\n",
    "    come up with your own mock data, several sentence pairs long. Try to construct datasets which illustrate problems of IBM Model1, e.g. garbage collection\n",
    "    \n",
    "    fun part: play with the Arcturan - Centauri data from lecture 1, to be found in data/dev-test-train-null.cen-act\n",
    "************************************************* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
